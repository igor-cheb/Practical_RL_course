{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in PyTorch\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb../xvfb: line 24: start-stop-daemon: command not found\r\n",
      ".\r\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 16:50:41.304 Python[7916:446198] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/org.python.python.savedState\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13f2ffe20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqk0lEQVR4nO3dfXBU9d3//9cmIZuEsBsTyC7RBGlrhVS8Aw0rtte3kouI0auWaIuT0qiMjDRQAUVMq3hXDUNnast1idSOFzijSMuM2MpVsDEoVgkQolhuJGKlJgqbWGl2A8jmZj+/P/rjtCs3siFkP4HnY+bMuOfzPrvv8xmz++LsOWddxhgjAAAAiyQlugEAAIAvIqAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOskNKA8+eSTOv/885WWlqaioiJt3rw5ke0AAABLJCyg/Pa3v9WcOXP04IMP6u2339Yll1yikpIStba2JqolAABgCVeifiywqKhIV1xxhf7nf/5HkhSNRpWfn6+ZM2fqvvvuS0RLAADAEimJeNGOjg41NDSoqqrKWZeUlKTi4mLV1dUdVR+JRBSJRJzH0WhU+/fvV05OjlwuV5/0DAAATo0xRu3t7crLy1NS0om/xElIQPn73/+u7u5u+Xy+mPU+n0+7du06qr66uloPP/xwX7UHAABOo+bmZp133nknrElIQIlXVVWV5syZ4zwOhUIqKChQc3OzPB5PAjsDAAAnKxwOKz8/X4MGDfrS2oQElMGDBys5OVktLS0x61taWuT3+4+qd7vdcrvdR633eDwEFAAA+pmTOT0jIVfxpKamavTo0aqtrXXWRaNR1dbWKhAIJKIlAABgkYR9xTNnzhxVVFRozJgxuvLKK/XLX/5SBw8e1G233ZaolgAAgCUSFlC+//3v69NPP9X8+fMVDAZ16aWXau3atUedOAsAAM4+CbsPyqkIh8Pyer0KhUKcgwIAQD8Rz+c3v8UDAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCduAPKG2+8oRtuuEF5eXlyuVx66aWXYsaNMZo/f76GDh2q9PR0FRcXa/fu3TE1+/fvV3l5uTwej7KysjR16lQdOHDglHYEAACcOeIOKAcPHtQll1yiJ5988pjjCxcu1KJFi7RkyRJt2rRJAwcOVElJiQ4fPuzUlJeXa8eOHaqpqdHq1av1xhtvaNq0aT3fCwAAcEZxGWNMjzd2ubRq1SrdeOONkv559CQvL09333237rnnHklSKBSSz+fTsmXLNHnyZL333nsqLCxUfX29xowZI0lau3atrrvuOn388cfKy8v70tcNh8Pyer0KhULyeDw9bR8AAPSheD6/e/UclD179igYDKq4uNhZ5/V6VVRUpLq6OklSXV2dsrKynHAiScXFxUpKStKmTZuO+byRSEThcDhmAQAAZ65eDSjBYFCS5PP5Ytb7fD5nLBgMKjc3N2Y8JSVF2dnZTs0XVVdXy+v1Okt+fn5vtg0AACzTL67iqaqqUigUcpbm5uZEtwQAAE6jXg0ofr9fktTS0hKzvqWlxRnz+/1qbW2NGe/q6tL+/fudmi9yu93yeDwxCwAAOHP1akAZPny4/H6/amtrnXXhcFibNm1SIBCQJAUCAbW1tamhocGpWbdunaLRqIqKinqzHQAA0E+lxLvBgQMH9MEHHziP9+zZo61btyo7O1sFBQWaNWuWfvazn+mCCy7Q8OHD9cADDygvL8+50mfkyJG69tprdccdd2jJkiXq7OzUjBkzNHny5JO6ggcAAJz54g4oW7Zs0be//W3n8Zw5cyRJFRUVWrZsme69914dPHhQ06ZNU1tbm66++mqtXbtWaWlpzjbPP/+8ZsyYofHjxyspKUllZWVatGhRL+wOAAA4E5zSfVAShfugAADQ/yTsPigAAAC9gYACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCduAJKdXW1rrjiCg0aNEi5ubm68cYb1djYGFNz+PBhVVZWKicnR5mZmSorK1NLS0tMTVNTk0pLS5WRkaHc3FzNnTtXXV1dp743AADgjBBXQFm/fr0qKyu1ceNG1dTUqLOzUxMmTNDBgwedmtmzZ+vll1/WypUrtX79eu3du1eTJk1yxru7u1VaWqqOjg5t2LBBzz77rJYtW6b58+f33l4BAIB+zWWMMT3d+NNPP1Vubq7Wr1+vb33rWwqFQhoyZIiWL1+um266SZK0a9cujRw5UnV1dRo7dqzWrFmj66+/Xnv37pXP55MkLVmyRPPmzdOnn36q1NTUo14nEokoEok4j8PhsPLz8xUKheTxeHraPgAA6EPhcFher/ekPr9P6RyUUCgkScrOzpYkNTQ0qLOzU8XFxU7NiBEjVFBQoLq6OklSXV2dRo0a5YQTSSopKVE4HNaOHTuO+TrV1dXyer3Okp+ffyptAwAAy/U4oESjUc2aNUvjxo3TRRddJEkKBoNKTU1VVlZWTK3P51MwGHRq/j2cHBk/MnYsVVVVCoVCztLc3NzTtgEAQD+Q0tMNKysrtX37dr355pu92c8xud1uud3u0/46AADADj06gjJjxgytXr1ar732ms477zxnvd/vV0dHh9ra2mLqW1pa5Pf7nZovXtVz5PGRGgAAcHaLK6AYYzRjxgytWrVK69at0/Dhw2PGR48erQEDBqi2ttZZ19jYqKamJgUCAUlSIBDQtm3b1Nra6tTU1NTI4/GosLDwVPYFAACcIeL6iqeyslLLly/X73//ew0aNMg5Z8Tr9So9PV1er1dTp07VnDlzlJ2dLY/Ho5kzZyoQCGjs2LGSpAkTJqiwsFBTpkzRwoULFQwGdf/996uyspKvcQAAgKQ4LzN2uVzHXL906VLdeuutkv55o7a7775bL7zwgiKRiEpKSrR48eKYr28++ugjTZ8+Xa+//roGDhyoiooKLViwQCkpJ5eX4rlMCQAA2CGez+9Tug9KohBQAADof/rsPigAAACnAwEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB14gooTz31lC6++GJ5PB55PB4FAgGtWbPGGT98+LAqKyuVk5OjzMxMlZWVqaWlJeY5mpqaVFpaqoyMDOXm5mru3Lnq6urqnb0BAABnhLgCynnnnacFCxaooaFBW7Zs0TXXXKPvfOc72rFjhyRp9uzZevnll7Vy5UqtX79ee/fu1aRJk5ztu7u7VVpaqo6ODm3YsEHPPvusli1bpvnz5/fuXgEAgH7NZYwxp/IE2dnZ+vnPf66bbrpJQ4YM0fLly3XTTTdJknbt2qWRI0eqrq5OY8eO1Zo1a3T99ddr79698vl8kqQlS5Zo3rx5+vTTT5WamnpSrxkOh+X1ehUKheTxeE6lfQAA0Efi+fzu8Tko3d3dWrFihQ4ePKhAIKCGhgZ1dnaquLjYqRkxYoQKCgpUV1cnSaqrq9OoUaOccCJJJSUlCofDzlGYY4lEIgqHwzELAAA4c8UdULZt26bMzEy53W7deeedWrVqlQoLCxUMBpWamqqsrKyYep/Pp2AwKEkKBoMx4eTI+JGx46murpbX63WW/Pz8eNsGAAD9SNwB5cILL9TWrVu1adMmTZ8+XRUVFdq5c+fp6M1RVVWlUCjkLM3Nzaf19QAAQGKlxLtBamqqvva1r0mSRo8erfr6ev3qV7/S97//fXV0dKitrS3mKEpLS4v8fr8kye/3a/PmzTHPd+QqnyM1x+J2u+V2u+NtFQAA9FOnfB+UaDSqSCSi0aNHa8CAAaqtrXXGGhsb1dTUpEAgIEkKBALatm2bWltbnZqamhp5PB4VFhaeaisAAOAMEdcRlKqqKk2cOFEFBQVqb2/X8uXL9frrr+uVV16R1+vV1KlTNWfOHGVnZ8vj8WjmzJkKBAIaO3asJGnChAkqLCzUlClTtHDhQgWDQd1///2qrKzkCAkAAHDEFVBaW1v1wx/+UPv27ZPX69XFF1+sV155Rf/5n/8pSXriiSeUlJSksrIyRSIRlZSUaPHixc72ycnJWr16taZPn65AIKCBAweqoqJCjzzySO/uFQAA6NdO+T4oicB9UAAA6H/65D4oAAAApwsBBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYJ67f4gGAU2WMkUxU3R2Hdeiz5qPGXUnJGpg7XEnJvD0BZzPeAQD0qYOtH+qjN56T6e5UJPzpUeMpaZkqvPkhJaUPSkB3AGxBQAHQp6KdER3+x95EtwHAcpyDAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAH1qQIZXyakZxx2Pdneqo/3vfdgRABsRUAD0KbfXpwEDvccdj3ZGdOjvTX3YEQAbEVAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAFgna7IIRljEt0GgAQioADoU66kZHnO+8YJa9r2vC2ZaB91BMBGBBQAfcrlcil14DknrOHoCQACCgAAsM4pBZQFCxbI5XJp1qxZzrrDhw+rsrJSOTk5yszMVFlZmVpaWmK2a2pqUmlpqTIyMpSbm6u5c+eqq6vrVFoBAABnkB4HlPr6ev3617/WxRdfHLN+9uzZevnll7Vy5UqtX79ee/fu1aRJk5zx7u5ulZaWqqOjQxs2bNCzzz6rZcuWaf78+T3fCwAAcEbpUUA5cOCAysvL9Zvf/EbnnPOv75JDoZCeeeYZ/eIXv9A111yj0aNHa+nSpdqwYYM2btwoSfrTn/6knTt36rnnntOll16qiRMn6tFHH9WTTz6pjo6O3tkrAADQr/UooFRWVqq0tFTFxcUx6xsaGtTZ2RmzfsSIESooKFBdXZ0kqa6uTqNGjZLP53NqSkpKFA6HtWPHjmO+XiQSUTgcjlkAAMCZKyXeDVasWKG3335b9fX1R40Fg0GlpqYqKysrZr3P51MwGHRq/j2cHBk/MnYs1dXVevjhh+NtFQAA9FNxHUFpbm7WXXfdpeeff15paWmnq6ejVFVVKRQKOUtzc3OfvTaA3peW5ZMr6fj/Puo63K7OQ6E+7AiAbeIKKA0NDWptbdXll1+ulJQUpaSkaP369Vq0aJFSUlLk8/nU0dGhtra2mO1aWlrk9/slSX6//6ireo48PlLzRW63Wx6PJ2YB0H9lDC5QUkrqccc7D7Yp0v5ZH3YEwDZxBZTx48dr27Zt2rp1q7OMGTNG5eXlzn8PGDBAtbW1zjaNjY1qampSIBCQJAUCAW3btk2tra1OTU1NjTwejwoLC3tptwAAQH8W1zkogwYN0kUXXRSzbuDAgcrJyXHWT506VXPmzFF2drY8Ho9mzpypQCCgsWPHSpImTJigwsJCTZkyRQsXLlQwGNT999+vyspKud3uXtotAADQn8V9kuyXeeKJJ5SUlKSysjJFIhGVlJRo8eLFznhycrJWr16t6dOnKxAIaODAgaqoqNAjjzzS260AAIB+ymX64Y9ehMNheb1ehUIhzkcB+qHOQyHt+N1D6u44dNyar99wjwYNvaAPuwJwusXz+c1v8QAAAOsQUAAAgHUIKAAs1e++fQbQiwgoAPpcsnugMv1fO2FN6KO/9FE3AGxEQAHQ55KSU5SSlnnCmo6DbX3TDAArEVAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAGQEEkDTvzr5ZFwq7o7Pu+jbgDYhoACICGyvzpGkuu444fbggQU4CxGQAGQEK6k5ES3AMBiBBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoACwljEm0S0ASBACCoCESEnLVEpa5nHHo10dCn+8sw87AmATAgqAhEgdlCO3Z/DxC4xR1+EDfdcQAKsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAZAgLiUNSDthRbTzMDdrA85SBBQACZP91TEnHG/721aZ7q4+6gaATQgoABLC5XJJrhO/BZloVBJHUICzUVwB5aGHHpLL5YpZRowY4YwfPnxYlZWVysnJUWZmpsrKytTS0hLzHE1NTSotLVVGRoZyc3M1d+5cdXXxLyQAAPAvKfFu8I1vfEOvvvrqv54g5V9PMXv2bP3f//2fVq5cKa/XqxkzZmjSpEl66623JEnd3d0qLS2V3+/Xhg0btG/fPv3whz/UgAED9Pjjj/fC7gAAgDNB3AElJSVFfr//qPWhUEjPPPOMli9frmuuuUaStHTpUo0cOVIbN27U2LFj9ac//Uk7d+7Uq6++Kp/Pp0svvVSPPvqo5s2bp4ceekipqanHfM1IJKJIJOI8DofD8bYNAAD6kbjPQdm9e7fy8vL0la98ReXl5WpqapIkNTQ0qLOzU8XFxU7tiBEjVFBQoLq6OklSXV2dRo0aJZ/P59SUlJQoHA5rx44dx33N6upqeb1eZ8nPz4+3bQAA0I/EFVCKioq0bNkyrV27Vk899ZT27Nmjb37zm2pvb1cwGFRqaqqysrJitvH5fAoGg5KkYDAYE06OjB8ZO56qqiqFQiFnaW5ujqdtAADQz8T1Fc/EiROd/7744otVVFSkYcOG6Xe/+53S09N7vbkj3G633G73aXt+AABgl1O6zDgrK0tf//rX9cEHH8jv96ujo0NtbW0xNS0tLc45K36//6ireo48PtZ5LQDObO5BOUpKOfa5Z5LU3fG5Og629V1DAKxxSgHlwIED+utf/6qhQ4dq9OjRGjBggGpra53xxsZGNTU1KRAISJICgYC2bdum1tZWp6ampkYej0eFhYWn0gqAfig9J1/Jqcc/+tp1uF2H2/b1YUcAbBHXVzz33HOPbrjhBg0bNkx79+7Vgw8+qOTkZN1yyy3yer2aOnWq5syZo+zsbHk8Hs2cOVOBQEBjx46VJE2YMEGFhYWaMmWKFi5cqGAwqPvvv1+VlZV8hQMAABxxBZSPP/5Yt9xyiz777DMNGTJEV199tTZu3KghQ4ZIkp544gklJSWprKxMkUhEJSUlWrx4sbN9cnKyVq9erenTpysQCGjgwIGqqKjQI4880rt7BQAA+jWX6Ye/xBUOh+X1ehUKheTxeBLdDoAe6ooc0s6VD6nzUOi4NV8t+ZGyhl3Sh10BOF3i+fzmt3gAAIB1CCgAAMA6BBQAAGAdAgoAq3VHDqkfnioH4BQRUAAkTPIAtzKHfv2ENfv/uqWPugFgEwIKgIRxJSVrQIb3xEUcPQHOSgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAJFRSSuoJxzsO7ld3x6E+6gaALQgoABLqnOGXy5WUfNzxSKhVXZ+392FHAGxAQAGQUK7klES3AMBCBBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANbh9HkAp+TAgQP661//2vMn+PwfMsYcd9gYo8bGRimtpccvkZSUpAsvvFCpqSe+5woAexBQAJyS+vp6TZgwocfbuwck68m7Juqi4bnHHI9Gu/WLB2dree22Hr/GwIEDtXXrVp1//vk9fg4AfYuAAuCUGGPU1dXV4+27urrUfijy/z+XdDg6UFGTpBRXl9zJnyvJ5VJWZuopvwaA/oWAAsAKXdEU7T5wuZo//7q6oqlKSz6kgoz3lJ/+fqJbA5AABBQACddtUrQzHFDT5yMkuSRJh7o92tV+pdo6h8iYTYltEECfI6AASLjPOvLU9m/h5F9caj1coLbOY5+fAuDMxWXGABJu7+df1dHh5J+iSlbw8Pl92g+AxCOgAEi46Anfio4dXACc2QgoABIuL+2vko53LxSj9OQDfdkOAAsQUAAkXIb5SEPTPtSxQoon5TN9xfuRXBxIAc4qcQeUTz75RD/4wQ+Uk5Oj9PR0jRo1Slu2bHHGjTGaP3++hg4dqvT0dBUXF2v37t0xz7F//36Vl5fL4/EoKytLU6dO1YED/AsJOFvVbnlP3xi0Xue635PLRBSNditFBzUsfasu876i/zdqiDLcAxLdJoA+FNdVPP/4xz80btw4ffvb39aaNWs0ZMgQ7d69W+ecc45Ts3DhQi1atEjPPvushg8frgceeEAlJSXauXOn0tLSJEnl5eXat2+fampq1NnZqdtuu03Tpk3T8uXLe3fvAPQLm3d9oqkLfiuj3+lgl1fdJkUDkiLKSG6XJHV1R/V5hJutAWcTlznRj2B8wX333ae33npLf/7zn485boxRXl6e7r77bt1zzz2SpFAoJJ/Pp2XLlmny5Ml67733VFhYqPr6eo0ZM0aStHbtWl133XX6+OOPlZeX96V9hMNheb1evfXWW8rMzDzZ9gGcBps3b9Ydd9yR6DZOKD09XS+++OJJvb8AOH0OHDigcePGKRQKyePxnLA2riMof/jDH1RSUqKbb75Z69ev17nnnqsf/ehHzpvTnj17FAwGVVxc7Gzj9XpVVFSkuro6TZ48WXV1dcrKynLCiSQVFxcrKSlJmzZt0ne/+92jXjcSiSgSiTiPw+GwJKm9vV3RaDSeXQDQyw4dOpToFr6UMUYHDhxw3jsAJMbBgwdPujaugPLhhx/qqaee0pw5c/STn/xE9fX1+vGPf6zU1FRVVFQoGAxKknw+X8x2Pp/PGQsGg8rNjb3pUkpKirKzs52aL6qurtbDDz981PpAIPClCQzA6dXR0ZHoFr5UcnKyxowZw48FAgkWzz8S4jpJNhqN6vLLL9fjjz+uyy67TNOmTdMdd9yhJUuWxN1kPKqqqhQKhZylubn5tL4eAABIrLgCytChQ1VYWBizbuTIkWpqapIk+f1+SVJLS0tMTUtLizPm9/vV2toaM97V1aX9+/c7NV/kdrvl8XhiFgAAcOaKK6CMGzdOjY2NMevef/99DRs2TJI0fPhw+f1+1dbWOuPhcFibNm1SIBCQ9M+vZdra2tTQ0ODUrFu3TtFoVEVFRT3eEQAAcOaI6xyU2bNn66qrrtLjjz+u733ve9q8ebOefvppPf3005Ikl8ulWbNm6Wc/+5kuuOAC5zLjvLw83XjjjZL+ecTl2muvdb4a6uzs1IwZMzR58mTOsAcAAJLiDChXXHGFVq1apaqqKj3yyCMaPny4fvnLX6q8vNypuffee3Xw4EFNmzZNbW1tuvrqq7V27VrnHiiS9Pzzz2vGjBkaP368kpKSVFZWpkWLFvXeXgEAgH4trvug2OLIfVBO5jpqAKfXunXrNH78+ES3cUIDBw7U9u3buYoHSLB4Pr/5LR4AAGAdAgoAALAOAQUAAFiHgAIAAKwT11U8APBFubm5+t73vpfoNk4oLS1NGRkZiW4DQBy4igcAAPQJruIBAAD9GgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgnbgCyvnnny+Xy3XUUllZKUk6fPiwKisrlZOTo8zMTJWVlamlpSXmOZqamlRaWqqMjAzl5uZq7ty56urq6r09AgAA/V5cAaW+vl779u1zlpqaGknSzTffLEmaPXu2Xn75Za1cuVLr16/X3r17NWnSJGf77u5ulZaWqqOjQxs2bNCzzz6rZcuWaf78+b24SwAAoL9zGWNMTzeeNWuWVq9erd27dyscDmvIkCFavny5brrpJknSrl27NHLkSNXV1Wns2LFas2aNrr/+eu3du1c+n0+StGTJEs2bN0+ffvqpUlNTT+p1w+GwvF6vQqGQPB5PT9sHAAB9KJ7P7x6fg9LR0aHnnntOt99+u1wulxoaGtTZ2ani4mKnZsSIESooKFBdXZ0kqa6uTqNGjXLCiSSVlJQoHA5rx44dx32tSCSicDgcswAAgDNXjwPKSy+9pLa2Nt16662SpGAwqNTUVGVlZcXU+Xw+BYNBp+bfw8mR8SNjx1NdXS2v1+ss+fn5PW0bAAD0Az0OKM8884wmTpyovLy83uznmKqqqhQKhZylubn5tL8mAABInJSebPTRRx/p1Vdf1Ysvvuis8/v96ujoUFtbW8xRlJaWFvn9fqdm8+bNMc915CqfIzXH4na75Xa7e9IqAADoh3p0BGXp0qXKzc1VaWmps2706NEaMGCAamtrnXWNjY1qampSIBCQJAUCAW3btk2tra1OTU1NjTwejwoLC3u6DwAA4AwT9xGUaDSqpUuXqqKiQikp/9rc6/Vq6tSpmjNnjrKzs+XxeDRz5kwFAgGNHTtWkjRhwgQVFhZqypQpWrhwoYLBoO6//35VVlZyhAQAADjiDiivvvqqmpqadPvttx819sQTTygpKUllZWWKRCIqKSnR4sWLnfHk5GStXr1a06dPVyAQ0MCBA1VRUaFHHnnk1PYCAACcUU7pPiiJwn1QAADof/rkPigAAACnCwEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOSqIb6AljjCQpHA4nuBMAAHCyjnxuH/kcP5F+GVA+++wzSVJ+fn6COwEAAPFqb2+X1+s9YU2/DCjZ2dmSpKampi/dQZxYOBxWfn6+mpub5fF4Et1Ov8Zc9g7msfcwl72Huewdxhi1t7crLy/vS2v7ZUBJSvrnqTNer5f/UXqJx+NhLnsJc9k7mMfew1z2Huby1J3sgQVOkgUAANYhoAAAAOv0y4Didrv14IMPyu12J7qVfo+57D3MZe9gHnsPc9l7mMu+5zInc60PAABAH+qXR1AAAMCZjYACAACsQ0ABAADWIaAAAADrEFAAAIB1+mVAefLJJ3X++ecrLS1NRUVF2rx5c6Jbskp1dbWuuOIKDRo0SLm5ubrxxhvV2NgYU3P48GFVVlYqJydHmZmZKisrU0tLS0xNU1OTSktLlZGRodzcXM2dO1ddXV19uStWWbBggVwul2bNmuWsYx5P3ieffKIf/OAHysnJUXp6ukaNGqUtW7Y448YYzZ8/X0OHDlV6erqKi4u1e/fumOfYv3+/ysvL5fF4lJWVpalTp+rAgQN9vSsJ1d3drQceeEDDhw9Xenq6vvrVr+rRRx+N+fE15vLY3njjDd1www3Ky8uTy+XSSy+9FDPeW/P2l7/8Rd/85jeVlpam/Px8LVy48HTv2pnJ9DMrVqwwqamp5n//93/Njh07zB133GGysrJMS0tLoluzRklJiVm6dKnZvn272bp1q7nuuutMQUGBOXDggFNz5513mvz8fFNbW2u2bNlixo4da6666ipnvKury1x00UWmuLjYvPPOO+aPf/yjGTx4sKmqqkrELiXc5s2bzfnnn28uvvhic9dddznrmceTs3//fjNs2DBz6623mk2bNpkPP/zQvPLKK+aDDz5wahYsWGC8Xq956aWXzLvvvmv+67/+ywwfPtx8/vnnTs21115rLrnkErNx40bz5z//2Xzta18zt9xySyJ2KWEee+wxk5OTY1avXm327NljVq5caTIzM82vfvUrp4a5PLY//vGP5qc//al58cUXjSSzatWqmPHemLdQKGR8Pp8pLy8327dvNy+88IJJT083v/71r/tqN88Y/S6gXHnllaaystJ53N3dbfLy8kx1dXUCu7Jba2urkWTWr19vjDGmra3NDBgwwKxcudKpee+994wkU1dXZ4z55x9yUlKSCQaDTs1TTz1lPB6PiUQifbsDCdbe3m4uuOACU1NTY/7jP/7DCSjM48mbN2+eufrqq487Ho1Gjd/vNz//+c+ddW1tbcbtdpsXXnjBGGPMzp07jSRTX1/v1KxZs8a4XC7zySefnL7mLVNaWmpuv/32mHWTJk0y5eXlxhjm8mR9MaD01rwtXrzYnHPOOTF/3/PmzTMXXnjhad6jM0+/+oqno6NDDQ0NKi4udtYlJSWpuLhYdXV1CezMbqFQSNK/fgW6oaFBnZ2dMfM4YsQIFRQUOPNYV1enUaNGyefzOTUlJSUKh8PasWNHH3afeJWVlSotLY2ZL4l5jMcf/vAHjRkzRjfffLNyc3N12WWX6Te/+Y0zvmfPHgWDwZi59Hq9KioqipnLrKwsjRkzxqkpLi5WUlKSNm3a1Hc7k2BXXXWVamtr9f7770uS3n33Xb355puaOHGiJOayp3pr3urq6vStb31LqampTk1JSYkaGxv1j3/8o4/25szQr37N+O9//7u6u7tj3uwlyefzadeuXQnqym7RaFSzZs3SuHHjdNFFF0mSgsGgUlNTlZWVFVPr8/kUDAadmmPN85Gxs8WKFSv09ttvq76+/qgx5vHkffjhh3rqqac0Z84c/eQnP1F9fb1+/OMfKzU1VRUVFc5cHGuu/n0uc3NzY8ZTUlKUnZ19Vs3lfffdp3A4rBEjRig5OVnd3d167LHHVF5eLknMZQ/11rwFg0ENHz78qOc4MnbOOeeclv7PRP0qoCB+lZWV2r59u958881Et9LvNDc366677lJNTY3S0tIS3U6/Fo1GNWbMGD3++OOSpMsuu0zbt2/XkiVLVFFRkeDu+pff/e53ev7557V8+XJ94xvf0NatWzVr1izl5eUxlzij9KuveAYPHqzk5OSjrpJoaWmR3+9PUFf2mjFjhlavXq3XXntN5513nrPe7/ero6NDbW1tMfX/Po9+v/+Y83xk7GzQ0NCg1tZWXX755UpJSVFKSorWr1+vRYsWKSUlRT6fj3k8SUOHDlVhYWHMupEjR6qpqUnSv+biRH/bfr9fra2tMeNdXV3av3//WTWXc+fO1X333afJkydr1KhRmjJlimbPnq3q6mpJzGVP9da88Tffe/pVQElNTdXo0aNVW1vrrItGo6qtrVUgEEhgZ3YxxmjGjBlatWqV1q1bd9ThxtGjR2vAgAEx89jY2KimpiZnHgOBgLZt2xbzx1hTUyOPx3PUB82Zavz48dq2bZu2bt3qLGPGjFF5ebnz38zjyRk3btxRl7q///77GjZsmCRp+PDh8vv9MXMZDoe1adOmmLlsa2tTQ0ODU7Nu3TpFo1EVFRX1wV7Y4dChQ0pKin3rTk5OVjQalcRc9lRvzVsgENAbb7yhzs5Op6ampkYXXnghX+/EK9Fn6cZrxYoVxu12m2XLlpmdO3eaadOmmaysrJirJM5206dPN16v17z++utm3759znLo0CGn5s477zQFBQVm3bp1ZsuWLSYQCJhAIOCMH7k8dsKECWbr1q1m7dq1ZsiQIWfd5bFf9O9X8RjDPJ6szZs3m5SUFPPYY4+Z3bt3m+eff95kZGSY5557zqlZsGCBycrKMr///e/NX/7yF/Od73znmJd4XnbZZWbTpk3mzTffNBdccMEZf2nsF1VUVJhzzz3Xucz4xRdfNIMHDzb33nuvU8NcHlt7e7t55513zDvvvGMkmV/84hfmnXfeMR999JExpnfmra2tzfh8PjNlyhSzfft2s2LFCpORkcFlxj3Q7wKKMcb893//tykoKDCpqanmyiuvNBs3bkx0S1aRdMxl6dKlTs3nn39ufvSjH5lzzjnHZGRkmO9+97tm3759Mc/zt7/9zUycONGkp6ebwYMHm7vvvtt0dnb28d7Y5YsBhXk8eS+//LK56KKLjNvtNiNGjDBPP/10zHg0GjUPPPCA8fl8xu12m/Hjx5vGxsaYms8++8zccsstJjMz03g8HnPbbbeZ9vb2vtyNhAuHw+auu+4yBQUFJi0tzXzlK18xP/3pT2Mua2Uuj+2111475ntjRUWFMab35u3dd981V199tXG73ebcc881CxYs6KtdPKO4jPm32w8CAABYoF+dgwIAAM4OBBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsM7/B1BgYQZbqeBMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "\n",
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('layer1', nn.Linear(state_dim[0], 64))\n",
    "model.add_module('layer2', nn.ReLU())\n",
    "\n",
    "model.add_module('layer3', nn.Linear(64, 32))\n",
    "model.add_module('layer4', nn.ReLU())\n",
    "\n",
    "model.add_module('layer5', nn.Linear(32, n_actions))\n",
    "\n",
    "# torch.nn.init.kaiming_uniform_(network[0].weight)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    \n",
    "    states_t = torch.tensor(states, requires_grad=True)\n",
    "    logits = model(states_t)\n",
    "    \n",
    "    return torch.nn.functional.softmax(logits).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_7916/3837099428.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(logits).detach().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "\n",
    "        a = random.choices(range(len(action_probs)), action_probs)[0]\n",
    "#         print(action_probs, a)\n",
    "#         a = np.argmax(action_probs)\n",
    "        \n",
    "#         print(action_probs, a)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_7916/3837099428.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(logits).detach().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99):  # discount for reward\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    all_rewards = []                  \n",
    "    cum_sum = 0\n",
    "    for r in rewards[::-1]:\n",
    "        cum_sum = r + gamma * cum_sum\n",
    "        all_rewards.append(cum_sum)\n",
    "    return all_rewards[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 2, 3, 4, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32, requires_grad=True)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states) \n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "    \n",
    "        \n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "    \n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "    \n",
    "    probs_for_actions = torch.sum(probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "#     print(log_probs_for_actions)\n",
    "\n",
    "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
    "    loss = -torch.mean(log_probs_for_actions * cumulative_returns ) - entropy_coef*torch.sum(probs_for_actions*log_probs_for_actions)\n",
    "\n",
    "    \n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_7916/3837099428.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(logits).detach().cpu().numpy()\n",
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_7916/2934456910.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  states = torch.tensor(states, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:25.590\n",
      "mean reward:33.160\n",
      "mean reward:40.000\n",
      "mean reward:25.870\n",
      "mean reward:21.360\n",
      "mean reward:29.110\n",
      "mean reward:41.050\n",
      "mean reward:68.660\n",
      "mean reward:118.170\n",
      "mean reward:123.840\n",
      "mean reward:213.850\n",
      "mean reward:240.940\n",
      "mean reward:102.460\n",
      "mean reward:89.160\n",
      "mean reward:88.170\n",
      "mean reward:114.450\n",
      "mean reward:174.800\n",
      "mean reward:108.120\n",
      "mean reward:115.360\n",
      "mean reward:661.090\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "    \n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    \n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_7916/3837099428.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(logits).detach().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/openaigym.video.1.7916.video000343.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical_rl_course",
   "language": "python",
   "name": "practical_rl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
