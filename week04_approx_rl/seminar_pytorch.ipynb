{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a __PyTorch__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb../xvfb: line 24: start-stop-daemon: command not found\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igorchebuniaev/.local/share/virtualenvs/Practical_RL_course-acchW1SA/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\").env #render=\"human\"\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())\n",
    "# plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gym\n",
    "import time\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Q-learning: building the network\n",
    "\n",
    "To train a neural network policy one must have a neural network policy. Let's build it.\n",
    "\n",
    "\n",
    "Since we're working with a pre-extracted features (cart positions, angles and velocities), we don't need a complicated network yet. In fact, let's build something like this for starters:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/yet_another_week/_resource/qlearning_scheme.png)\n",
    "\n",
    "For your first run, please only use linear layers (`nn.Linear`) and activations. Stuff like batch normalization or dropout may ruin everything if used haphazardly. \n",
    "\n",
    "Also please avoid using nonlinearities like sigmoid & tanh: since agent's observations are not normalized, sigmoids might be saturated at initialization. Instead, use non-saturating nonlinearities like ReLU.\n",
    "\n",
    "Ideally you should start small with maybe 1-2 hidden layers with < 200 neurons and then increase network size if agent doesn't beat the target score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential()\n",
    "\n",
    "network.add_module('layer1', nn.Linear(state_dim[0], 64))\n",
    "network.add_module('layer2', nn.ReLU())\n",
    "\n",
    "network.add_module('layer3', nn.Linear(64, 32))\n",
    "network.add_module('layer4', nn.ReLU())\n",
    "\n",
    "network.add_module('layer5', nn.Linear(32, n_actions))\n",
    "\n",
    "# torch.nn.init.kaiming_uniform_(network[0].weight)\n",
    "opt = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "# hint: use state_dim[0] as input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    state = torch.tensor(state[None], dtype=torch.float32)\n",
    "    q_values = network(state).detach().numpy()\n",
    "\n",
    "    if random.random() <= epsilon:\n",
    "        chosen_action = random.choice(range(n_actions))\n",
    "    else:\n",
    "        chosen_action = np.argmax(q_values)\n",
    "#         print(q_values, chosen_action)\n",
    "\n",
    "    return int(chosen_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_20797/2125760689.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  tmp_tns = torch.tensor([s]*3, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0437, 0.0437, 0.0437])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()[0]\n",
    "tmp_tns = torch.tensor([s]*3, dtype=torch.float32)\n",
    "torch.max(tmp_tns, dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0198, 0.0234, 0.0437, 0.0216],\n",
       "        [0.0198, 0.0234, 0.0437, 0.0216],\n",
       "        [0.0198, 0.0234, 0.0437, 0.0216]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n",
      "e=0.5 tests passed\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()[0]\n",
    "assert tuple(network(torch.tensor([s]*3, dtype=torch.float32)).size()) == (\n",
    "    3, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert isinstance(list(network.modules(\n",
    "))[-1], nn.Linear), \"please make sure you predict q-values without nonlinearity (ignore if you know what you're doing)\"\n",
    "assert isinstance(get_action(\n",
    "    s), int), \"get_action(s) must return int, not %s. try int(action)\" % (type(get_action(s)))\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount(\n",
    "        [get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] -\n",
    "               10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] -\n",
    "                       10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed' % eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning via gradient descent\n",
    "\n",
    "We shall now train our agent's Q-function by minimizing the TD loss:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
    "\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "The tricky part is with  $Q_{-}(s',a')$. From an engineering standpoint, it's the same as $Q_{\\theta}$ - the output of your neural network policy. However, when doing gradient descent, __we won't propagate gradients through it__ to make training more stable (see lectures).\n",
    "\n",
    "To do so, we shall use `x.detach()` function which basically says \"consider this thing constant when doing backprop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "    states = torch.tensor(\n",
    "        states, dtype=torch.float32)                                  # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)                 # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)              # shape: [batch_size]\n",
    "    # shape: [batch_size, state_size]\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    is_done = torch.tensor(is_done, dtype=torch.uint8)                # shape: [batch_size]\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = network(states)                               # shape: [batch_size, n_actions]\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[                # shape: [batch_size]\n",
    "      range(states.shape[0]), actions\n",
    "    ]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = network(next_states)\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = torch.max(predicted_next_qvalues, dim=1).values\n",
    "#     assert next_state_values.dtype == torch.float32\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    target_qvalues_for_actions = (rewards + gamma * next_state_values.detach())\n",
    "\n",
    "    # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    target_qvalues_for_actions = torch.where(\n",
    "        is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_20797/675179679.py:30: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  target_qvalues_for_actions = torch.where(\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "s = env.reset()[0]\n",
    "a = env.action_space.sample()\n",
    "next_s, r, done, _, _ = env.step(a)\n",
    "loss = compute_td_loss([s], [a], [r], [next_s], [done], check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert len(loss.size()) == 0, \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(network.parameters()).grad.detach().numpy() !=\n",
    "              0), \"loss must be differentiable w.r.t. network weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000, epsilon=0, train=False, show=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()[0]\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)\n",
    "        next_s, r, done, _, _ = env.step(a)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            compute_td_loss([s], [a], [r], [next_s], [done]).backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        if show:\n",
    "            env.render()\n",
    "            time.sleep(.1)\n",
    "            clear_output(wait=True)\n",
    "    env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_22330/675179679.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  states = torch.tensor(\n",
      "/var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/ipykernel_22330/675179679.py:30: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  target_qvalues_for_actions = torch.where(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 14.407\tepsilon = 0.500\n",
      "epoch #10\tmean reward = 40.433\tepsilon = 0.452\n",
      "epoch #20\tmean reward = 156.573\tepsilon = 0.409\n",
      "epoch #30\tmean reward = 448.227\tepsilon = 0.370\n",
      "epoch #40\tmean reward = 509.447\tepsilon = 0.334\n",
      "epoch #50\tmean reward = 208.893\tepsilon = 0.303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3000\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     session_rewards \u001b[38;5;241m=\u001b[39m [generate_session(env, epsilon\u001b[38;5;241m=\u001b[39mepsilon, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m150\u001b[39m)]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch #\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mmean reward = \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mepsilon = \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, np\u001b[38;5;241m.\u001b[39mmean(session_rewards), epsilon))\n",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3000\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     session_rewards \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m150\u001b[39m)]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch #\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mmean reward = \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mepsilon = \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, np\u001b[38;5;241m.\u001b[39mmean(session_rewards), epsilon))\n",
      "Cell \u001b[0;32mIn [7], line 13\u001b[0m, in \u001b[0;36mgenerate_session\u001b[0;34m(env, t_max, epsilon, train, show)\u001b[0m\n\u001b[1;32m     11\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m     compute_td_loss([s], [a], [r], [next_s], [done])\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m     16\u001b[0m s \u001b[38;5;241m=\u001b[39m next_s\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Practical_RL_course-acchW1SA/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Practical_RL_course-acchW1SA/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Practical_RL_course-acchW1SA/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Practical_RL_course-acchW1SA/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Practical_RL_course-acchW1SA/lib/python3.10/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    session_rewards = [generate_session(env, epsilon=epsilon, train=True, show=False) for _ in range(150)]\n",
    "    if i%10 == 0:\n",
    "        print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "\n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "\n",
    "    if np.mean(session_rewards) > 5000:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret results\n",
    "\n",
    "\n",
    "Welcome to the f.. world of deep f...n reinforcement learning. Don't expect agent's reward to smoothly go up. Hope for it to go increase eventually. If it deems you worthy.\n",
    "\n",
    "Seriously though,\n",
    "* __ mean reward__ is the average reward per game. For a correct implementation it may stay low for some 10 epochs, then start growing while oscilating insanely and converges by ~50-100 steps depending on the network architecture. \n",
    "* If it never reaches target score by the end of for loop, try increasing the number of hidden neurons or look at the epsilon.\n",
    "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at < 0.01 epsilon before it's is at least 200, just reset it back to 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record videos\n",
    "\n",
    "As usual, we now use `gym.wrappers.Monitor` to record a video of our agent playing the game. Unlike our previous attempts with state binarization, this time we expect our agent to act ~~(or fail)~~ more smoothly since there's no more binarization error at play.\n",
    "\n",
    "As you already did with tabular q-learning, we set epsilon=0 for final evaluation to prevent agent from exploring himself to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igorchebuniaev/.local/share/virtualenvs/Practical_RL_course-acchW1SA/lib/python3.10/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/igorchebuniaev/Documents/Обучение/RL/Practical_RL_course/week04_approx_rl/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/igorchebuniaev/Documents/Обучение/RL/Practical_RL_course/week04_approx_rl/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/igorchebuniaev/Documents/Обучение/RL/Practical_RL_course/week04_approx_rl/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/igorchebuniaev/Documents/Обучение/RL/Practical_RL_course/week04_approx_rl/video/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\").env #render=\"human\"\n",
    "# env.reset()\n",
    "\n",
    "env = gym.wrappers.RecordVideo(env, 'video')\n",
    "generate_session(env, t_max=1000, epsilon=0, train=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical_rl_course",
   "language": "python",
   "name": "practical_rl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
